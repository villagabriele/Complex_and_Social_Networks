---
title: "Lab03_CSN_final"
author: "Gabriele Villa, Giulia Pinciroli"
date: "2025-10-24"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Setup of libraries and clusters

```{r}
library(igraph)
library(dplyr)
library(parallel)
library(udpipe)
library(stats4)
library(VGAM)
library(ggplot2)
library(gridExtra)
library(xtable)
```

```{r}
# stop any stray cluster object if present
if (exists("cl") && inherits(cl, "cluster")) {
  try(parallel::stopCluster(cl), silent = TRUE)
  rm(cl)
}

# close any lingering connections
try(closeAllConnections(), silent = TRUE)
```

```{r}
# Windows parallel setup 
library(parallel)

ncores <- max(1, parallel::detectCores() - 1)
cl <- parallel::makeCluster(ncores)

# Load needed packages on workers
parallel::clusterEvalQ(cl, {
  library(igraph)
  library(dplyr)
  library(udpipe)
})
```

# Network Creation and Summary Table

### Create the Network

Main idea: create a list containing in each index an igraph network object, each representing the linguistic dependency structures for one corpus file-This allows us to access it through its index knowing the order of the languages.

The computation was parallelized using clusters to make the process run faster.

```{r}
graphs=list()
base_dir <- "./pud"
files <- list.files(path = base_dir,
                    pattern = "pud-ud-test\\.conllu$",
                    recursive = TRUE,
                    full.names = TRUE)
create.graph.sequence <- function(){
  parse_sentence <- function(block) {
    token_lines <- grep("^[0-9]+\\t", block, value = TRUE)
    if (length(token_lines) == 0) return(NULL)
    df <- read.table(text = token_lines, sep = "\t", header = FALSE, quote = "", fill = TRUE)
    
    colnames(df)[1:8] <- c("ID", "FORM", "LEMMA", "UPOS", "XPOS", "FEATS", "HEAD", "DEPREL")
    df <- df[!grepl("-", df$ID), ]
    df$ID <- as.numeric(df$ID)
    df$HEAD <- as.numeric(df$HEAD)
    return(df)
  }
  
  # export objects used inside workers
  parallel::clusterExport(cl, c("files", "parse_sentence"), envir = environment())
  
  graphs <- parallel::parLapply(cl, files, function(f) {
    lines <- readLines(f, encoding = "UTF-8")
    # Sentence splitting
    sentence_blocks <- split(lines, cumsum(grepl("^# sent_id", lines)))
    
    sentences <- lapply(sentence_blocks, parse_sentence)
    sentences <- sentences[!sapply(sentences, is.null)]
    
    # extract language name
    parent_folder <- basename(dirname(f))
    lang_name <- sub("^UD_(.*)-PUD$", "\\1", parent_folder)
    
    edges_all <- do.call(rbind, lapply(sentences, function(df) {
      col_to_use <- if (lang_name %in% c("Portuguese", "Korean")) "FORM" else "LEMMA"
      id_map <- setNames(df[[col_to_use]], df$ID)           # map token ID -> lemma/form (built BEFORE filtering)
      df2 <- df[df$HEAD != 0, c("ID", "HEAD")]              # dependents only
      if (NROW(df2) == 0) return(NULL)
      head_value  <- unname(id_map[as.character(df2$HEAD)])
      token_value <- unname(id_map[as.character(df2$ID)])
      data.frame(head_value = head_value, token_value = token_value, stringsAsFactors = FALSE)
    }))
    
    
    edges_all <- stats::na.omit(edges_all)
    edges_all <- unique(edges_all)
    g <- igraph::graph_from_data_frame(edges_all, directed = FALSE)
    igraph::simplify(g, remove.loops = TRUE, remove.multiple = TRUE)
    
  })
  
  return(graphs)
}
```

```{r}
list_of_graphs <- create.graph.sequence()
```

```{r}
languages <- c("Arabic", "Chinese", "Czech", "English", "Finnish", "French", 
               "Galician", "German", "Hindi", "Icelandic", "Indonesian", 
               "Italian", "Japanese", "Korean", "Polish", "Portuguese", 
               "Russian", "Spanish", "Swedish", "Thai", "Turkish")
```

### Properties of the degree sequences

```{r}
# Output directory
output_dir <- "./data"

# Create 'data' folder if it doesn't exist already
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# Function
save_graph_files <- function(graph_list, lang_names, output_directory) {
  
  for (i in 1:length(graph_list)) {
    g <- graph_list[[i]]
    lang_name <- lang_names[i]
    
    print(paste("Saving:", lang_name))
    
    # Extract graph information
    n_vertices <- vcount(g)
    n_edges <- ecount(g)
    
    # Extract edges list
    edge_list <- as_edgelist(g)
    
    # Name of the output file
    output_file <- file.path(output_directory, 
                             paste0(lang_name, "_dependency_network.txt"))
    
    # Create the file
    con <- file(output_file, "w", encoding = "UTF-8")
    
    # Header: number of vertice and edges
    writeLines(paste(n_vertices, n_edges), con)
    
    # List of the edges
    for (j in 1:nrow(edge_list)) {
      writeLines(paste(edge_list[j, 1], edge_list[j, 2]), con)
    }
    
    close(con)
    
    print(paste("  -> Vertices:", n_vertices, "- Edges:", n_edges))
  }
}

# Function
save_graph_files(list_of_graphs, languages, output_dir)
```

### Table with summary of results

```{r}
# Function to calculate graph properties and create summary table
create_summary_table <- function(graph_list, lang_names) {
  parallel::clusterExport(cl, c("graph_list"), envir = environment())
  
  stats_list <- parallel::parLapply(cl, graph_list, function(g) {
    N <- igraph::vcount(g)
    E <- igraph::ecount(g)
    k_mean <- (2 * E) / N
    delta  <- (2 * E) / (N * (N - 1))
    c(N = N, E = E, k_mean = k_mean, delta = delta)
  })
  
  stats_mat <- do.call(rbind, stats_list)
  
  summary_table <- data.frame(
    Language = lang_names,
    N = stats_mat[, "N"],
    E = stats_mat[, "E"],
    k_mean = round(stats_mat[, "k_mean"], 3),
    delta  = round(stats_mat[, "delta"], 5)
  )
  colnames(summary_table) <- c("Language", "N", "E", "<k>", "Î´")
  return(summary_table)
}

# Create the summary table
table_1 <- create_summary_table(list_of_graphs, languages)

# Save the table to a text file
write.table(table_1, file = "summary_table.txt", sep = "\t", row.names = FALSE, quote = FALSE)

# Display the table
print(table_1)

```

# Implementations of Closeness Centrality

### 1. Calculate Closeness Centrality of the original languages graphs through igraph package *harmonic_centrality*

```{r}
# Function 1: using harmonic_centrality from igraph

mean_closeness <- function(g) {
  N <- vcount(g)
  if (N <= 1) return(0)
  

  # harmonic_centrality with parameter "normalized" = TRUE calculates the mean of sum(1/d_ij), automatically taking care of 1/Inf = 0.
  harmonic_values <- harmonic_centrality(g, mode = "all", normalize = TRUE)
  
  # Mean of all the harmonic centrality scores of all the vertices in a graph.
  mean_C <- mean(harmonic_values)
  
  return(mean_C)
}

```

```{r}

# Calculate for all graphs

parallel::clusterExport(cl, "mean_closeness", envir = environment())
start_h <- Sys.time()
mean_closeness_all <- unlist(parallel::parLapply(cl, list_of_graphs, mean_closeness))
end_h <- Sys.time()
time_h <- as.numeric(difftime(end_h, start_h, units = "secs"))

print(mean_closeness_all)
print(paste("Time elapsed:", time_h ))
```

### Evaluating performance of different methods on smallest graph

Generate graphs to be used to test the efficiency of the methods.

```{r}
example_graph <- sample_gnm(n=table_1[table_1$Language == 'Indonesian', "N"], m = table_1[table_1$Language == 'Indonesian', "E"])

E <- table_1[table_1$Language == 'Indonesian', "E"]
Q <- max(10, 2 * log(E))
example_graph_switching <- rewire(
      list_of_graphs[[11]],
      with = keeping_degseq(niter = round(Q * E), loops = FALSE)
    )
```

### 1. Performance of Harmonic Centrality

```{r}
start_h <- Sys.time()
mean_closeness_per <- mean_closeness(example_graph)
end_h <- Sys.time()
time_h <- as.numeric(difftime(end_h, start_h, units = "secs"))
print(paste("Time elapsed:", time_h ))
```

```{r}
start_h_switch <- Sys.time()
mean_closeness_per_switch <- mean_closeness(example_graph_switching)
end_h_switch <- Sys.time()
time_h_switch <- as.numeric(difftime(end_h_switch, start_h_switch, units = "secs"))
print(paste("Time elapsed:", time_h_switch ))
```

### 2. Manual implementation of the function to calculate Closeness Centrality yielding approximate results.

Implementing the function manually allows us to select only a subset of nodes, therefore arriving at an approximated result.\
As suggested by the assignment, randomly selecting only 10% of the nodes already yield a very good approximation.\
Setting the parameter "*sample_fraction*" to 1 will give the same results as the implementation with the igraph function.

Distances will be calculated with BFS, implemented through the igraph function *bfs.*

```{r}

# Function 2: manual calculation through BFS
closeness_manual <- function(g, sample_fraction) {
  N <- vcount(g)
  
  # nodes selection
  if (sample_fraction == 1) {
    sampled_vertices <- 1:N
  } else {
    sampled_vertices <- sample(1:N, size = floor(sample_fraction * N), replace = FALSE)
  }
  
  closeness_values <- numeric(length(sampled_vertices))
  
  for(idx in 1:length(sampled_vertices)) {
    i <- sampled_vertices[idx]
    
    # Checking for isolated nodes: if degree = 0, then C_i = 0
    if(degree(g, v = i, mode = "all") == 0) {
      closeness_values[idx] <- 0
      next
    }
    
    # BFS to calculate distances
    bfs_result <- bfs(g, root = i, dist = TRUE, unreachable = TRUE, mode = "all")
    distances <- bfs_result$dist[-i]
    
    # Calculate inverses: 1/d_ij if finite and >0, otherwise 0
    reciprocals <- ifelse(is.finite(distances) & distances > 0, 1/distances, 0)
    
    # C_i = (1/(N-1)) * sum(1/d_ij)
    closeness_values[idx] <- (1 / (N - 1)) * sum(reciprocals, na.rm = TRUE)
  }
  
  return(mean(closeness_values))
}

```

#### Performance of model with sample approximation

```{r}
start_m <- Sys.time()
closeness_manual_per <- closeness_manual(example_graph, sample_fraction = 0.1)
end_m <- Sys.time()
time_m <- as.numeric(difftime(end_m, start_m, units = "secs"))
print(paste("Time elapsed:", time_m ))
```

```{r}
start_m_switch <- Sys.time()
closeness_manual_per_switch <- closeness_manual(example_graph_switching, sample_fraction = 0.1)
end_m_switch <- Sys.time()
time_m_switch <- as.numeric(difftime(end_m_switch, start_m_switch, units = "secs"))
print(paste("Time elapsed:", time_m_switch ))
```

### 3. Degree-1 optimization keeping results exact

Through the use of a cache, store breadth-first search results to avoid redundant calculations. In the case of degree-1 nodes, check if their only neighbor's BFS distances are already cached to then add 1 to the retrieved values.

```{r}
closeness_manual_optimized <- function(g) {
  N <- vcount(g)
  
  sampled_vertices <- 1:N
  
  # Pre-calculate all degrees only once
  degrees <- degree(g, v = sampled_vertices, mode = "all")
  
  closeness_values <- numeric(length(sampled_vertices))
  bfs_cache <- list()
  
  for(idx in 1:length(sampled_vertices)) {
    i <- sampled_vertices[idx]
    deg_i <- degrees[idx]
    
    # Check isolated nodes and give them closeness centrality 0
    if(deg_i == 0) {
      closeness_values[idx] <- 0
      next
    }
    
    # Optimization: if degree = 1
    if(deg_i == 1) {
      neighbor <- neighbors(g, v = i, mode = "all")
      k <- as.integer(neighbor[1])
      
      cache_key <- as.character(k)
      
      # Use values stored in chace only if available, otherwise calculate from i
      
      if(!is.null(bfs_cache[[cache_key]])) {
        distances <- bfs_cache[[cache_key]] + 1
        distances[i] <- 0
        distances <- distances[-i]
      } else {
        
        # More efficient to calculate form i then from the closest neighbour in this case
        bfs_result <- bfs(g, root = i, dist = TRUE, unreachable = TRUE, mode = "all")
        distances <- bfs_result$dist[-i]
        
        bfs_cache[[as.character(i)]] <- bfs_result$dist
      }
      
    } else {
      # Normal case
      bfs_result <- bfs(g, root = i, dist = TRUE, unreachable = TRUE, mode = "all")
      distances <- bfs_result$dist[-i]
      
      # Save in cache
      bfs_cache[[as.character(i)]] <- bfs_result$dist
    }
    
    # Calculate reciprocals 
    reciprocals <- ifelse(is.finite(distances) & distances > 0, 1/distances, 0)
    closeness_values[idx] <- (1 / (N - 1)) * sum(reciprocals, na.rm = TRUE)
  }
  
  return(mean(closeness_values))
}
```

#### Performance of model with Degree-1 optimization

```{r}
start_o <- Sys.time()
closeness_manual_optimized_per <- closeness_manual_optimized(example_graph)
end_o <- Sys.time()
time_o <- as.numeric(difftime(end_o, start_o, units = "secs"))
print(paste("Time elapsed:", time_o ))
```

```{r}
start_o_switch <- Sys.time()
closeness_manual_optimized_per_switch <- closeness_manual_optimized(example_graph_switching)
end_o_switch <- Sys.time()
time_o_switch <- as.numeric(difftime(end_o_switch, start_o_switch, units = "secs"))
print(paste("Time elapsed:", time_m_switch ))
```

### 4. Early stopping with bounds optimization keeping results exact

#### Parameter selection for early stopping

```{r}
findM = function(g, possible_M, language_closeness, lang) {
    m=1
    N <- vcount(g)
    
    vertex_degrees <- degree(g)
    ordered_vertices <- order(vertex_degrees, decreasing = FALSE)
    
    # Nodes selection
    sampled_vertices <- ordered_vertices
    
    closeness_values <- numeric(length(sampled_vertices))
    count=0
    
    for(idx in 1:length(sampled_vertices)) {
      
      i <- sampled_vertices[idx]
      count=count+1
      
      # Check isolated nodes: if degree = 0, then C_i = 0
      if(degree(g, v = i) == 0) {
        closeness_values[idx] <- 0
        next
      }
      # BFS to calculate distances
      bfs_result <- bfs(g, root = i, dist = TRUE)
      distances <- bfs_result$dist[-i]
      
      # Calculate the reciprocals: 1/d_ij if finite and > 0, else 0
      reciprocals <- ifelse(is.finite(distances) & distances > 0, 1/distances, 0)
      
      # C_i = (1/(N-1)) * sum(1/d_ij)
      closeness_values[idx] <- (1 / (N - 1)) * sum(reciprocals)
      
      if (count==possible_M[m]) {
        cmax=mean(closeness_values) + 1 - possible_M[m]/N
        

        cmin=mean(closeness_values)
        

        if (cmin > language_closeness) {
           return(possible_M[m])
        }
       


         if (cmax <= language_closeness) {
           return (possible_M[m])
         }
        m <- m + 1
        if(m > length(possible_M)) {
        return('You reached the maximum M')
    }
        
      }
    }
    
    return('You reached the maximum M')
  }
```

```{r}
set.seed(1234)
seeds <- sample.int(.Machine$integer.max, length(languages))

clusterExport(
  cl,
  c("findM", "languages", "table_1", "mean_closeness_all", "seeds"),
  envir = environment()
)


# Compute M_values in parallel (one task per language)
M_values_list <- parLapply(cl, seq_along(languages), function(i) {
  lang <- languages[i]
  N <- table_1[table_1$Language == lang, "N"]
  E <- table_1[table_1$Language == lang, "E"]

  # Use the same seed for this language regardless of worker
  set.seed(seeds[i])
  g <- sample_gnm(n = N, m = E)

  possible_M <- round(seq(from = 0.8, to = 1, by = 0.05) * N)


  findM(g, possible_M, mean_closeness_all[i], lang)
})

# Flatten to your original M_values vector
M_values <- unlist(M_values_list, use.names = FALSE)
```

```{r}
print(M_values)
```

```{r}
closeness_manual_exact <- function(g, M, language_closeness, decr) {
  
  N <- vcount(g)
  
  vertex_degrees <- degree(g)
  ordered_vertices <- order(vertex_degrees, decreasing = decr)
  
  # Node selection
  sampled_vertices <- ordered_vertices
  
  closeness_values <- numeric(length(sampled_vertices))
  count=0
  
  for(idx in 1:length(sampled_vertices)) {
    
    i <- sampled_vertices[idx]
    count=count+1
    
    # Check isolated nodes: if degree = 0, then C_i = 0
    if(degree(g, v = i) == 0) {
      closeness_values[idx] <- 0
      next
    }
    
    # BFS to calculate distances
    bfs_result <- bfs(g, root = i, dist = TRUE)
    distances <- bfs_result$dist[-i]
    
    # Calculate reciprocals: 1/d_ij if finite and > 0, otherwise 0
    reciprocals <- ifelse(is.finite(distances) & distances > 0, 1/distances, 0)
    
    # C_i = (1/(N-1)) * sum(1/d_ij)
    closeness_values[idx] <- (1 / (N - 1)) * sum(reciprocals)
    
    if (count==M) {

      cmax=mean(closeness_values) + 1 - M/N
      cmin=mean(closeness_values)

      if (cmin >= language_closeness) {
        return (1)
      }
       if (cmax <= language_closeness) {
         return (0)
       }
      
    }
  }
  
  return(mean(closeness_values))
}
```

#### Performance of model with early stopping with bounds

```{r}
start_e <- Sys.time()
closeness_manual_exact_per <- closeness_manual_exact(example_graph, M_values[11], mean_closeness_all[11], FALSE)
end_e <- Sys.time()
time_e <- as.numeric(difftime(end_e, start_e, units = "secs"))
print(paste("Time elapsed:", time_e ))
```

```{r}
start_e_switch <- Sys.time()
closeness_manual_exact_per_switch <- closeness_manual_exact(example_graph_switching, M_values[11], mean_closeness_all[11], TRUE)
end_e_switch <- Sys.time()
time_e_switch <- as.numeric(difftime(end_e_switch, start_e_switch, units = "secs"))
print(paste("Time elapsed:", time_e_switch ))
```

### 5. Optimization using both M and cache

```{r}

calculate_closeness_manual_optimized_threshold <- function(g, M, language_closeness, decr) {
  N <- vcount(g)
  
  # Degrees and low-to-high ordering 
  vertex_degrees <- degree(g)
  ordered_vertices <- order(vertex_degrees, decreasing = decr)
  sampled_vertices <- ordered_vertices
  degrees <- vertex_degrees[sampled_vertices]
  
  closeness_values <- numeric(length(sampled_vertices))
  bfs_cache <- list()
  count <- 0
  
  for (idx in 1:length(sampled_vertices)) {
    i <- sampled_vertices[idx]
    deg_i <- degrees[idx]
    count <- count + 1
    
    # Isolated nodes -> closeness 0
    if (deg_i == 0) {
      closeness_values[idx] <- 0
    } else if (deg_i == 1) {
      # Degree-1 optimization with cache 
      neighbor <- neighbors(g, v = i, mode = "all")
      k <- as.integer(neighbor[1])
      cache_key <- as.character(k)
      
      if (!is.null(bfs_cache[[cache_key]])) {
        distances <- bfs_cache[[cache_key]] + 1
        distances[i] <- 0
        distances <- distances[-i]
      } else {
        # More efficient to calculate from i than from the closest neighbour in this case
        bfs_result <- bfs(g, root = i, dist = TRUE, unreachable = TRUE, mode = "all")
        distances <- bfs_result$dist[-i]
        bfs_cache[[as.character(i)]] <- bfs_result$dist
      }
      
      reciprocals <- ifelse(is.finite(distances) & distances > 0, 1 / distances, 0)
      closeness_values[idx] <- (1 / (N - 1)) * sum(reciprocals, na.rm = TRUE)
      
    } else {
      # Normal case with cache storage 
      bfs_result <- bfs(g, root = i, dist = TRUE, unreachable = TRUE, mode = "all")
      distances <- bfs_result$dist[-i]
      bfs_cache[[as.character(i)]] <- bfs_result$dist
      
      reciprocals <- ifelse(is.finite(distances) & distances > 0, 1 / distances, 0)
      closeness_values[idx] <- (1 / (N - 1)) * sum(reciprocals, na.rm = TRUE)
    }
    
    # Early stopping check using  M-threshold logic
    if (count == M) {

      cmax <- mean(closeness_values[1:count]) + 1 - M / N
      cmin <- mean(closeness_values[1:count])

      
      if (cmin >= language_closeness) {
        return(1)
      }
      if (cmax <= language_closeness) {
        return(0)
      }
      # otherwise continue to finish the exact mean over all nodes
    }
  }
  
  # If no early decision, return the mean closeness over all nodes 
  return(mean(closeness_values))
}
```

#### Performance of model using combined optimization

```{r}
start_ot <- Sys.time()
closeness_manual_optimized_threshold_per <- calculate_closeness_manual_optimized_threshold(example_graph, M_values[11], mean_closeness_all[11], FALSE)
end_ot <- Sys.time()
time_ot <- as.numeric(difftime(end_ot, start_ot, units = "secs"))
print(paste("Time elapsed:", time_ot ))
```

```{r}
start_ot_switch <- Sys.time()
closeness_manual_optimized_threshold_per_switch <- calculate_closeness_manual_optimized_threshold(example_graph_switching, M_values[11], mean_closeness_all[11], TRUE)
end_ot_switch <- Sys.time()
time_ot_switch <- as.numeric(difftime(end_ot_switch, start_ot_switch, units = "secs"))
print(paste("Time elapsed:", time_ot_switch ))
```

### Create tables to compare performance of models

```{r}
time_table <- data.frame(
  Model = c("Harmonic Centrality (igraph)",
            "Approximated Manual BFS",
            "Exact Manual with Cache",
            "Exact Manual with M Threshold",
            "Exact Manual with Cache and M"),
  "Performance on ER graph (seconds)" = c(time_h, time_m, time_o, time_e, time_ot),
  "Performance on switching model graph (seconds)" = c(time_h_switch, time_m_switch, time_o_switch, time_e_switch, time_ot_switch)
)
colnames(time_table) <- c("Model", "Performance on ER graph (seconds)", "Performance on switching model graph (seconds)")
# Print the table in console
print(time_table)

# Save the table to a .txt file (tab-separated)
write.table(
  time_table,
  file = "model_times.txt",
  sep = "\t",
  row.names = FALSE,
  quote = FALSE
)
```

### Having observed the performances of the various models. we conclude that the built -in R function Harmonic Centrality is the most efficient. It will be therefore used to test for significance.

### Erdos-Renyi graph

Main idea: for each language, create *T_val* times a gnm graph with the same number of nodes and edges of the original graph. Then, at each iteration, calculate the closeness centrality and store those values in a list called *mean_closeness_values*.

Count how many times the closeness centrality in the ER graph is greater than the closeness centrality of the original graphs.

After *T_val* times, take the average of the values to calculate the p-value.

```{r}
T_val <- 500


# Precompute a deterministic seed schedule: one vector of seeds per language
set.seed(12345)  # <- change this to change the whole experiment reproducibly
seeds_list <- lapply(seq_along(languages), function(.) {
  sample.int(.Machine$integer.max, T_val, replace = FALSE)
})

# Worker function: compute p-value for one language index
.compute_p_for_lang <- function(idx) {
  lang <- languages[idx]
  N <- table_1[table_1$Language == lang, "N"]
  E <- table_1[table_1$Language == lang, "E"]

  mean_closeness_values <- numeric(T_val)
  for (i in seq_len(T_val)) {
    set.seed(seeds_list[[idx]][i])  # fixed seed for (lang idx, iteration i)
    g <- sample_gnm(n = N, m = E)
    mean_closeness_values[i] <- mean_closeness(g)
  }

  p_value <- sum(mean_closeness_values > mean_closeness_all[idx]) / T_val
  data.frame(Language = lang, p_value = p_value, stringsAsFactors = FALSE)
}


clusterExport(
  cl,
  c("languages", "table_1", "T_val",
    "mean_closeness_all", "seeds_list", "mean_closeness")
)
t_start = Sys.time()
pvals_list <- parLapply(cl, seq_along(languages), .compute_p_for_lang)
t_end = Sys.time()
time_elapsed = as.numeric(difftime(t_end, t_start, units = "secs"))
pvals_parallel_binomial <- do.call(rbind, pvals_list)

print(pvals_parallel_binomial)
print(time_elapsed)

```

### Switching Model

For each language, generate *T_val* graphs performing a degree-preserving random rewiring of the edges.\
For each iteration, the original graph is rewired with *Q* set to *max(10, 2log(E))* times the number of edges, to ensure enough swaps to randomize while preserving the degree sequence.

Count how many times the rewired graph's closeness centrality is greater than the closeness centrality of the original graphs.

After *T_val* times, take the average of the values to calculate the p-value.

```{r}
T_val <- 500

# Export all needed objects and functions to the cluster
parallel::clusterExport(
  cl,
  c("languages", "list_of_graphs", "table_1", "mean_closeness_all",
    "T_val", "mean_closeness", "keeping_degseq"),
  envir = environment()
)

# Define the worker task
.compute_p_for_lang_rewire <- function(idx) {
  lang <- languages[idx]
  E <- table_1[table_1$Language == lang, "E"]
  Q <- max(10, 2 * log(E))
  mean_closeness_values <- numeric(T_val)

  for (i in seq_len(T_val)) {
    # Random rewire of each graph
    set.seed(i + idx * 10000)  # seed for reproducibility
    g <- rewire(
      list_of_graphs[[idx]],
      with = keeping_degseq(niter = round(Q * E), loops = FALSE)
    )
    mean_closeness_values[i] <- mean_closeness(g)
  }

  # Compute p-value
  p_value <- sum(mean_closeness_values > mean_closeness_all[idx]) / T_val
  data.frame(Language = lang, p_value = p_value, stringsAsFactors = FALSE)
}

# Run in parallel over all languages (outer loop)
t_start = Sys.time()
pvals_list <- parallel::parLapply(cl, seq_along(languages), .compute_p_for_lang_rewire)
t_end = Sys.time()
time_elapsed = as.numeric(difftime(t_end, t_start, units = "secs"))
pvals_parallel_rewire <- do.call(rbind, pvals_list)

# Show results
print(pvals_parallel_rewire)
```

```{r}
final_table <- data.frame(
  Language = languages,
  "Closeness Centrality" = mean_closeness_all,
  "p-value (binomial)" = sprintf("%.4f", pvals_parallel_binomial$p_value),
  "p-value (switching)" = sprintf("%.4f", pvals_parallel_rewire$p_value),
  stringsAsFactors = FALSE
)
colnames(final_table) <- c("Language", "Closeness Centrality", "p-value (binomial)", "p-value (switching)")

# Save as txt (tab-separated)
write.table(final_table, file = "pvalues_table.txt", sep = "\t", row.names = FALSE, quote = FALSE)
print(final_table)
```

```{r}
#stop clusters previously started
parallel::stopCluster(cl)
```
